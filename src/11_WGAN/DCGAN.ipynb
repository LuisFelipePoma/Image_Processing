{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGcCsOilc8b1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (Reshape,LeakyReLU,Dropout,Conv2DTranspose, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, )\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPLHB2s5gBJu"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKsQaul6le_"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnE86Hpn6GGQ",
        "outputId": "26f29d5f-c29a-4410-bcd9-7246b925a911"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "!unzip \"/content/celeba-dataset.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV3KzHNq6w0g"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7pb2bpv6GLf"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "IM_SHAPE = (64,64,3)\n",
        "LEARNING_RATE = 2e-4\n",
        "LATENT_DIM=100\n",
        "EPOCHS=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLcX0RFw4leA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNiSHjva6GN5",
        "outputId": "230059a4-30f8-4a13-b717-ff60bf3fbdf2"
      },
      "outputs": [],
      "source": [
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/dataset/img_align_celeba/img_align_celeba\", label_mode=None, image_size=(IM_SHAPE[0], IM_SHAPE[1]), batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrd47dQJ4muu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V98GV8FA4eYO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT7fSwhF6GQj",
        "outputId": "3b36a037-2e3a-4b6c-9d62-5f6dc0f78b45"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBO55qltL0OS"
      },
      "outputs": [],
      "source": [
        "def preprocess(image):\n",
        "  '''hacer que la imagen esté en el rango [-1,1]\n",
        "  '''\n",
        "  return tf.cast(image, tf.float32) / 127.5 - 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfOhaJz-6GSw"
      },
      "outputs": [],
      "source": [
        "train_dataset = (\n",
        "    dataset\n",
        "    .map(preprocess)\n",
        "    .unbatch()\n",
        "    .shuffle(buffer_size = 1024, reshuffle_each_iteration = True)\n",
        "    .batch(BATCH_SIZE,drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU4bl2Hd6GYE",
        "outputId": "d1ac239d-90c9-4ca0-caa6-e65b62280c45"
      },
      "outputs": [],
      "source": [
        "# veamos un batch\n",
        "for d in train_dataset.take(1):\n",
        "  print(d.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "RcTlClDN6Gaq",
        "outputId": "e777ea3f-4df0-43f4-94c9-454afece3f06"
      },
      "outputs": [],
      "source": [
        "# veamos las imágenes...\n",
        "plt.figure(figsize = (6,6))\n",
        "k=0\n",
        "n = 4\n",
        "for i in range(n):\n",
        "  ax = plt.subplot(2,2, k+1)\n",
        "  plt.imshow((d[i]+1)/2)\n",
        "  plt.axis(\"off\")\n",
        "  k+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdtWfpUGgC4E"
      },
      "source": [
        "# MOdeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni3A9hDmgKKU"
      },
      "outputs": [],
      "source": [
        "# protocolo de pruebas...\n",
        "#  1. probar eliminando leaky RELU\n",
        "#  2. reemplazar Leaky RELU por ReLU:\n",
        "#     Conv2DTranspose(512,kernel_size=4,strides=2, padding='same', activation = 'relu')\n",
        "generator=tf.keras.Sequential([\n",
        "  Input(shape=(LATENT_DIM,)),\n",
        "  Dense(4*4*LATENT_DIM),\n",
        "  Reshape((4,4,LATENT_DIM)),\n",
        "\n",
        "  Conv2DTranspose(512,kernel_size=4,strides=2, padding='same'),\n",
        "  BatchNormalization(),\n",
        "  LeakyReLU(0.2),\n",
        "\n",
        "  Conv2DTranspose(256,kernel_size=4,strides=2, padding='same'),\n",
        "  BatchNormalization(),\n",
        "  LeakyReLU(0.2),\n",
        "\n",
        "  Conv2DTranspose(128,kernel_size=4,strides=2, padding='same'),\n",
        "  BatchNormalization(),\n",
        "  LeakyReLU(0.2),\n",
        "\n",
        "  Conv2DTranspose(3,kernel_size=4,strides=2, activation=tf.keras.activations.tanh, padding='same'),\n",
        "\n",
        "],name='generator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfYErd5YuEjg",
        "outputId": "edd3788b-41d4-4b77-f1aa-564e7883377d"
      },
      "outputs": [],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXiiUmhZuEmJ"
      },
      "outputs": [],
      "source": [
        "discriminator=tf.keras.Sequential([\n",
        "  Input(shape=(IM_SHAPE[0],IM_SHAPE[1],3)),\n",
        "\n",
        "  Conv2D(64,kernel_size=4,strides=2, padding='same'),\n",
        "  LeakyReLU(0.2),\n",
        "\n",
        "  Conv2D(128,kernel_size=4,strides=2, padding='same'),\n",
        "  BatchNormalization(),\n",
        "  LeakyReLU(0.2),\n",
        "\n",
        "  Conv2D(256,kernel_size=4,strides=2, padding='same'),\n",
        "  BatchNormalization(),\n",
        "  LeakyReLU(0.2),\n",
        "\n",
        "  Conv2D(1,kernel_size=4,strides=2, padding='same'),\n",
        "\n",
        "  Flatten(),\n",
        "  Dense(1,activation='sigmoid')\n",
        "\n",
        "\n",
        "],name='discriminator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxJiYAOquEon",
        "outputId": "0aa9fca5-ae0e-49a8-8857-535bb6e8089e"
      },
      "outputs": [],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsC-wtWGOU2X"
      },
      "outputs": [],
      "source": [
        "class ShowImage(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, latent_dim=100):\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        n=6\n",
        "        k=0\n",
        "        out=self.model.generator(tf.random.normal(shape=(36, self.latent_dim)))\n",
        "        plt.figure(figsize=(16,16))\n",
        "        for i in range(n):\n",
        "          for j in range(n):\n",
        "            ax=plt.subplot(n,n,k+1)\n",
        "            plt.imshow((out[k]+1)/2,)\n",
        "            plt.axis('off')\n",
        "            k+=1\n",
        "        plt.savefig(\"generated/gen_images_epoch_{}.png\".format(epoch+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcPT8TgBuErO"
      },
      "outputs": [],
      "source": [
        "class GAN(tf.keras.Model):\n",
        "  def __init__(self,discriminator,generator):\n",
        "    super(GAN,self).__init__()\n",
        "    self.discriminator=discriminator\n",
        "    self.generator=generator\n",
        "\n",
        "  def compile(self,d_optimizer,g_optimizer,loss_fn):\n",
        "    super(GAN,self).compile()\n",
        "    self.d_optimizer=d_optimizer\n",
        "    self.g_optimizer=g_optimizer\n",
        "    self.loss_fn=loss_fn\n",
        "    self.d_loss_metric=tf.keras.metrics.Mean(name='d_loss')\n",
        "    self.g_loss_metric=tf.keras.metrics.Mean(name='g_loss')\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [self.d_loss_metric,self.g_loss_metric]\n",
        "\n",
        "  def train_step(self,real_images):\n",
        "    batch_size=tf.shape(real_images)[0]\n",
        "\n",
        "    ######## Discriminator\n",
        "    random_noise=tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
        "    fake_images=self.generator(random_noise)\n",
        "\n",
        "    real_labels=tf.ones((batch_size,1))+0.25*tf.random.uniform((batch_size,1),minval=-1,maxval=1)\n",
        "    fake_labels=tf.zeros((batch_size,1))+0.25*tf.random.uniform((batch_size,1),)\n",
        "\n",
        "    with tf.GradientTape() as recorder:\n",
        "      real_predictions=self.discriminator(real_images)\n",
        "      d_loss_real=self.loss_fn(real_labels,real_predictions)\n",
        "\n",
        "      fake_predictions=self.discriminator(fake_images)\n",
        "      d_loss_fake=self.loss_fn(fake_labels,fake_predictions)\n",
        "\n",
        "      d_loss=d_loss_real+d_loss_fake\n",
        "\n",
        "    partial_derivatives = recorder.gradient(d_loss,self.discriminator.trainable_weights)\n",
        "    self.d_optimizer.apply_gradients(zip(partial_derivatives, self.discriminator.trainable_weights))\n",
        "\n",
        "    ############# Generator\n",
        "    random_noise=tf.random.normal(shape=(batch_size,LATENT_DIM))\n",
        "    flipped_fake_labels=tf.ones((batch_size,1))\n",
        "\n",
        "    with tf.GradientTape() as recorder:\n",
        "\n",
        "      fake_predictions=self.discriminator(self.generator(random_noise))\n",
        "      g_loss=self.loss_fn(flipped_fake_labels,fake_predictions)\n",
        "\n",
        "    partial_derivatives = recorder.gradient(g_loss,self.generator.trainable_weights)\n",
        "    self.g_optimizer.apply_gradients(zip(partial_derivatives, self.generator.trainable_weights))\n",
        "\n",
        "    self.d_loss_metric.update_state(d_loss)\n",
        "    self.g_loss_metric.update_state(g_loss)\n",
        "\n",
        "    return {'d_loss':self.d_loss_metric.result(),\n",
        "            'g_loss':self.g_loss_metric.result()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aoneKPouEtr"
      },
      "outputs": [],
      "source": [
        "gan=GAN(discriminator,generator)\n",
        "gan.compile(\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,beta_1=0.5),\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,beta_1=0.5),\n",
        "    loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHcLh4YeRF3I"
      },
      "outputs": [],
      "source": [
        "!mkdir generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFfDNRj8uEwi",
        "outputId": "39ea58d7-01c3-49fc-d13d-29846113cf6d"
      },
      "outputs": [],
      "source": [
        "EPOCHS=1000\n",
        "history=gan.fit(train_dataset,epochs=EPOCHS,callbacks=[ShowImage(LATENT_DIM)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPxKc48PuEzD"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['d_loss'])\n",
        "plt.plot(history.history['g_loss'])\n",
        "plt.title('GAN Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['d_loss', 'g_loss'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hDY8ORVJX3z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j-bpKlbwuE1j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLGNQSpXuE9N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjePWMBQuE_5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
